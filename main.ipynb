{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) #2.6.0+cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "print(\"device \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: include in application for MLI programme.\n",
    "personal contextual information\n",
    "I have built and trained convlutional networks in the past using keras + tensorflow.\n",
    "the inital stage of the project for me is more about re-familiarising myself with fundamentals and adapting to a new framework for underlying principles.\n",
    "'''\n",
    "\n",
    "# Define a transform to convert the data to tensor and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the training dataset with normalization\n",
    "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test dataset with normalization\n",
    "test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loader for the training dataset\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create data loader for the test dataset\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize images\n",
    "def show_images(images, labels):\n",
    "    fig, axes = plt.subplots(len(images)//5, 5, figsize=(10, 2))\n",
    "    for img, label, ax in zip(images, labels, axes):\n",
    "        ax.imshow(img.squeeze(), cmap='gray')\n",
    "        ax.set_title(f'Value: {label}')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of images from the training data loader\n",
    "data_iter = iter(train_loader)\n",
    "pictures, true_values = next(data_iter)\n",
    "\n",
    "# Display the first 5 images\n",
    "show_images(pictures[:5], true_values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrefresher on cnn fundamentals [https://www.youtube.com/watch?v=pDdP0TFzsoQ]\\nbuilding a cnn in pytorch.\\nconvolutional layers, pooling layers, rectified linear unit (ReLU) layers, and fully connected layer\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "refresher on cnn fundamentals [https://www.youtube.com/watch?v=pDdP0TFzsoQ]\n",
    "building a cnn in pytorch.\n",
    "convolutional layers, pooling layers, rectified linear unit (ReLU) layers, and fully connected layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining variables that will be used to build model and specifying processor for training\n",
    "classifications = ('0','1','2','3','4','5','6','7','8','9')\n",
    "batch_size = 64 #redeclared batch size so I can alter it for lighter load on training.\n",
    "# training_epochs = 100 #declared in 'training' section of file. here for reference\n",
    "learn_rate = 0.001\n",
    "\n",
    "#I have cuda-12.4 enabled GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)  # Should return a version number, not None\n",
    "print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#building conv model \n",
    "#MNIST dataset contains single-channel image -> Input 28*28\n",
    "# ( (Conv -> ReLU)*2 -> Pooling )*3 -> Fully Connected \n",
    "class ConvNet(nn.ModuleList):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(-1, 28 * 28)  # Flatten the input\n",
    "        inputs = self.fc1(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.fc2(inputs)\n",
    "        return self.softmax(inputs)\n",
    "\n",
    "\n",
    "model = ConvNet().to(device) #instance of class passed to GPU/CPU\n",
    "# model = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss() #CEL for classification model\n",
    "optimiser = optim.SGD(params=model.parameters(), lr=learn_rate)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 100\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    #iterate over train set and pass into model\n",
    "    # best_model_state = deepcopy(model.state_dict())  \n",
    "    for i, (image, label) in enumerate(train_loader):\n",
    "        # Get a batch of images from the training data loader\n",
    "        images = image.to(device)\n",
    "        labels = label.to(device)\n",
    "\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./saved_models/mid_train_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 94.300%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    # show_predictions(images.to(\"cpu\"),outputs.to(\"cpu\"),labels.to(\"cpu\"))\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.3f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pictures, true_values = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pred: 2, conf: 0.969 , true: 2)\n",
      "(pred: 2, conf: 1.000 , true: 2)\n",
      "(pred: 9, conf: 0.850 , true: 9)\n",
      "(pred: 1, conf: 0.762 , true: 1)\n",
      "(pred: 3, conf: 0.989 , true: 3)\n"
     ]
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(model(pictures[:5].to(device)), dim=1)\n",
    "\n",
    "for i in range(0,len(pictures[:5])):\n",
    "    conf, classes = torch.max(probs, 1) #1/0 for last arg\n",
    "    print(f\"(pred: {classifications[classes[i].item()]}, conf: {conf[i].item():.3f} , true: {true_values[i]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Plot some example test data with predictions and ground truths."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
